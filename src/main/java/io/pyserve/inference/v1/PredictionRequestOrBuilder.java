// Generated by the protocol buffer compiler.  DO NOT EDIT!
// NO CHECKED-IN PROTOBUF GENCODE
// source: inference.proto
// Protobuf Java Version: 4.31.0

package io.pyserve.inference.v1;

@com.google.protobuf.Generated
public interface PredictionRequestOrBuilder extends
    // @@protoc_insertion_point(interface_extends:io.pyserve.inference.v1.PredictionRequest)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * required
   * </pre>
   *
   * <code>string model_name = 1;</code>
   * @return The modelName.
   */
  java.lang.String getModelName();
  /**
   * <pre>
   * required
   * </pre>
   *
   * <code>string model_name = 1;</code>
   * @return The bytes for modelName.
   */
  com.google.protobuf.ByteString
      getModelNameBytes();

  /**
   * <pre>
   * optional
   * </pre>
   *
   * <code>string model_version = 2;</code>
   * @return The modelVersion.
   */
  java.lang.String getModelVersion();
  /**
   * <pre>
   * optional
   * </pre>
   *
   * <code>string model_version = 2;</code>
   * @return The bytes for modelVersion.
   */
  com.google.protobuf.ByteString
      getModelVersionBytes();

  /**
   * <pre>
   * Input data for model prediction
   * </pre>
   *
   * <code>map&lt;string, .io.pyserve.inference.v1.TorchTensorProto&gt; inputs = 3;</code>
   */
  int getInputsCount();
  /**
   * <pre>
   * Input data for model prediction
   * </pre>
   *
   * <code>map&lt;string, .io.pyserve.inference.v1.TorchTensorProto&gt; inputs = 3;</code>
   */
  boolean containsInputs(
      java.lang.String key);
  /**
   * Use {@link #getInputsMap()} instead.
   */
  @java.lang.Deprecated
  java.util.Map<java.lang.String, io.pyserve.inference.v1.Tensor.TorchTensorProto>
  getInputs();
  /**
   * <pre>
   * Input data for model prediction
   * </pre>
   *
   * <code>map&lt;string, .io.pyserve.inference.v1.TorchTensorProto&gt; inputs = 3;</code>
   */
  java.util.Map<java.lang.String, io.pyserve.inference.v1.Tensor.TorchTensorProto>
  getInputsMap();
  /**
   * <pre>
   * Input data for model prediction
   * </pre>
   *
   * <code>map&lt;string, .io.pyserve.inference.v1.TorchTensorProto&gt; inputs = 3;</code>
   */
  /* nullable */
io.pyserve.inference.v1.Tensor.TorchTensorProto getInputsOrDefault(
      java.lang.String key,
      /* nullable */
io.pyserve.inference.v1.Tensor.TorchTensorProto defaultValue);
  /**
   * <pre>
   * Input data for model prediction
   * </pre>
   *
   * <code>map&lt;string, .io.pyserve.inference.v1.TorchTensorProto&gt; inputs = 3;</code>
   */
  io.pyserve.inference.v1.Tensor.TorchTensorProto getInputsOrThrow(
      java.lang.String key);
}
