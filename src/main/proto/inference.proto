syntax = "proto3";

package io.pyserve.inference.v1;

import "tensor.proto";
import "google/rpc/status.proto";

option java_multiple_files = true;

message PredictionRequest {
    string model_name = 1; //required

    string model_version = 2; //optional

    // Input data for model prediction
    map<string, TorchTensorProto> inputs = 3;
}

message PredictionResponse {

	// Output tensors.
  	map<string, TorchTensorProto> outputs = 1;
	
	optional google.rpc.Status status = 2; //optional
}

service InferenceService {
 	rpc Predict(PredictionRequest) returns (PredictionResponse) {}
}